from typing import List, Dict, Set
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from difflib import SequenceMatcher

from logger.logger import setup_logger

logger = setup_logger(module_name=__name__)



class DeduplicationService:
    """
    –ú–æ–¥—É–ª—å deduplicator - —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π.

    –û—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –ø–æ URL –∏ —Å—Ö–æ–¥—Å—Ç–≤—É —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
    –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –¥–∞–π–¥–∂–µ—Å—Ç–µ.
    """
    def normalize_url(self, url: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —É–¥–∞–ª—è—è UTM-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –¥—Ä—É–≥–∏–µ tracking –¥–∞–Ω–Ω—ã–µ.

        Args:
            url: –ò—Å—Ö–æ–¥–Ω—ã–π URL

        Returns:
            str: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL
        """
        try:
            parsed = urlparse(url)

            # –£–¥–∞–ª—è–µ–º tracking –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            query_params = parse_qs(parsed.query)
            tracking_params = {
                'utm_campaign', 'utm_source', 'utm_medium', 'utm_term', 'utm_content',
                'fbclid', 'gclid', 'ref', 'source', 'campaign'
            }

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ-tracking –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            clean_params = {k: v for k, v in query_params.items()
                        if k.lower() not in tracking_params}

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º URL –±–µ–∑ tracking –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            clean_query = urlencode(clean_params, doseq=True)
            normalized = urlunparse((
                parsed.scheme.lower(),
                parsed.netloc.lower(),
                parsed.path.rstrip('/'),
                parsed.params,
                clean_query,
                ''  # —É–±–∏—Ä–∞–µ–º fragment
            ))

            return normalized

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ URL {url}: {e}")
            return url.lower().strip()


    def calculate_content_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏.

        Args:
            text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            float: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –æ—Ç 0 –¥–æ 1
        """
        if not text1 or not text2:
            return 0.0

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        normalized1 = text1.lower().strip()
        normalized2 = text2.lower().strip()

        return SequenceMatcher(None, normalized1, normalized2).ratio()


    def find_duplicates(self, articles: List[Dict[str, str]],
                    similarity_threshold: float = 0.85,
                    check_content: bool = True) -> List[Dict[str, str]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –∏ —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π.

        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π —Å –ø–æ–ª—è–º–∏ title, url, summary
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (0-1)
            check_content: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –ø–æ–º–∏–º–æ URL

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        if not articles:
            return []

        seen_urls: Set[str] = set()
        unique_articles: List[Dict[str, str]] = []
        duplicates_found = 0

        for article in articles:
            url = article.get('url', '')
            title = article.get('title', '')
            summary = article.get('summary', '')

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            normalized_url = self.normalize_url(url)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            if normalized_url in seen_urls:
                duplicates_found += 1
                logger.debug(f"–î—É–±–ª–∏–∫–∞—Ç –ø–æ URL –Ω–∞–π–¥–µ–Ω: {title[:50]}... ({url})")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏
            is_duplicate = False
            if check_content and unique_articles:
                for existing_article in unique_articles:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    title_similarity = self.calculate_content_similarity(
                        title, existing_article.get('title', '')
                    )

                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                    content_similarity = self.calculate_content_similarity(
                        summary, existing_article.get('summary', '')
                    )

                    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ - —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç
                    max_similarity = max(title_similarity, content_similarity)
                    if max_similarity >= similarity_threshold:
                        duplicates_found += 1
                        logger.debug(f"–î—É–±–ª–∏–∫–∞—Ç –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –Ω–∞–π–¥–µ–Ω (—Å—Ö–æ–¥—Å—Ç–≤–æ {max_similarity:.2f}): {title[:50]}...")
                        is_duplicate = True
                        break

            if not is_duplicate:
                seen_urls.add(normalized_url)
                unique_articles.append(article)

        logger.info(f"üîç –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: —É–¥–∞–ª–µ–Ω–æ {duplicates_found} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        return unique_articles


    def deduplicate_articles(self, articles: List[Dict[str, str]],
                            similarity_threshold: float = 0.85,
                            check_content: bool = True) -> List[Dict[str, str]]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ —Å—Ç–∞—Ç–µ–π.

        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            check_content: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è

        Returns:
            List[Dict]: –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        """
        logger.debug(f"–ù–∞—á–∏–Ω–∞–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é {len(articles)} —Å—Ç–∞—Ç–µ–π")

        unique_articles = self.find_duplicates(
            articles=articles,
            similarity_threshold=similarity_threshold,
            check_content=check_content
        )

        removed_count = len(articles) - len(unique_articles)
        if removed_count > 0:
            logger.info(f"‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {removed_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ, {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ—Å—Ç–∞–ª–æ—Å—å")
        else:
            logger.info(f"‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, {len(unique_articles)} —Å—Ç–∞—Ç–µ–π")

        return unique_articles