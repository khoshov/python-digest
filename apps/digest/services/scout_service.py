import requests
import feedparser

from datetime import datetime, timedelta
from urllib.parse import urlparse
from typing import List, Dict, Optional
import ssl
import urllib3


from newspaper import Article  # –Ω–µ —É–≤–∏–¥–µ–ª —Ç–∞–∫–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
from logger.logger import setup_logger

logger = setup_logger(module_name=__name__)


# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL –¥–ª—è RSS –ª–µ–Ω—Ç
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤-–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–µ–¥—É–µ—Ç –∏—Å–∫–ª—é—á–∞—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∞ –ª–∏—à—å —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
BLOCKED_DOMAINS = {"news.google.com", "news.ycombinator.com"}


class ScoutService:
    """
    –ê–≥–µ–Ω—Ç Scout - –º–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
    - –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ Google Custom Search API
    - –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π –∏–∑ RSS-–ª–µ–Ω—Ç
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç–µ–π
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """

    def __init__(self):
        pass

    def is_valid_article(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —Å—Ç–∞—Ç—å–∏.

        –ò—Å–∫–ª—é—á–∞–µ—Ç –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏ –¥—Ä—É–≥–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã.
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        Returns:
            bool: True, –µ—Å–ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º, False - –µ—Å–ª–∏ –Ω–µ—Ç
        """
        domain = urlparse(url).netloc
        return domain not in BLOCKED_DOMAINS

    def search_google_news(
        self, query: str, api_key: str, cse_id: str, num_results: int = 5
    ) -> List[Dict[str, str]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Google Custom Search API.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å (–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ)
            api_key: –ö–ª—é—á Google API
            cse_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã Google
            num_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'title' –∏ 'link'
        """
        url = "https://www.googleapis.com/customsearch/v1"

        # –£–ø—Ä–æ—â–∞–µ–º –∑–∞–ø—Ä–æ—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è Google API
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä—É—Å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏
        query_mapping = {
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç": "artificial intelligence",
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ": "machine learning",
            "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏": "neural networks",
            "Python —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞": "Python development",
            "Python –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã": "Python tools",
            "Python –±–∏–±–ª–∏–æ—Ç–µ–∫–∏": "Python libraries",
            "Python —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏": "Python frameworks",
            "Python –æ–±—É—á–µ–Ω–∏–µ": "Python tutorial",
            "Python –º–µ–º—ã": "Python programming",
            "Python –∏—Å—Ç–æ—Ä–∏—è": "Python news",
        }

        # –ó–∞–º–µ–Ω—è–µ–º —Ä—É—Å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ
        english_query = query_mapping.get(query, query)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        params = {
            "q": english_query,  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            "cx": cse_id,  # ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
            "key": api_key,  # API –∫–ª—é—á
            "num": min(
                num_results, 10
            ),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–º–∞–∫—Å 10 –¥–ª—è Google API)
            "sort": "date",  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
            "dateRestrict": "w1",  # –¢–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é (7 –¥–Ω–µ–π)
        }

        try:
            resp = requests.get(url, params=params, timeout=10)
            resp.raise_for_status()

            items = resp.json().get("items", [])

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏—Å–∫–ª—é—á–∞—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
            filtered_results = [
                {"title": item["title"], "link": item["link"]}
                for item in items
                if self.is_valid_article(item["link"])
            ]

            logger.debug(
                f"Google Search: –Ω–∞–π–¥–µ–Ω–æ {len(filtered_results)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{english_query}' (–æ—Ä–∏–≥–∏–Ω–∞–ª: '{query}')"
            )
            return filtered_results

        except requests.exceptions.RequestException as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ Google API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{english_query}': {e}"
            )
            return []
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{english_query}': {e}")
            return []

    def fetch_rss_headlines(
        self, feed_url: str, hours: int = 12
    ) -> List[Dict[str, str]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS-–ª–µ–Ω—Ç—ã –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.

        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.

        Args:
            feed_url: URL RSS-–ª–µ–Ω—Ç—ã
            hours: –ü–µ—Ä–∏–æ–¥ –≤ —á–∞—Å–∞—Ö –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 12)

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'title' –∏ 'link'
        """
        try:
            logger.debug(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ RSS: {feed_url}")

            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º SSL –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏
            if hasattr(ssl, "_create_unverified_context"):
                ssl._create_default_https_context = ssl._create_unverified_context

            # –ü–∞—Ä—Å–∏–º RSS-–ª–µ–Ω—Ç—É —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            feed = feedparser.parse(
                feed_url, agent="Mozilla/5.0 (compatible; RSS Reader)"
            )

            if feed.bozo:
                logger.warning(
                    f"RSS –ª–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏: {feed_url}, –æ—à–∏–±–∫–∞: {feed.bozo_exception}"
                )
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—à–∏–±–∫—É, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                if not feed.entries:
                    logger.warning(f"RSS –ª–µ–Ω—Ç–∞ {feed_url} –ø—É—Å—Ç–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return []

            logger.debug(f"RSS {feed_url}: –Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π –≤—Å–µ–≥–æ")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            cutoff = datetime.now() - timedelta(hours=hours)
            news = []

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∑–∞–ø–∏—Å—å –≤ –ª–µ–Ω—Ç–µ
            for entry in feed.entries:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
                    published = getattr(entry, "published_parsed", None)
                    if not published:
                        # –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç, –±–µ—Ä–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É (–¥–ª—è —Å–≤–µ–∂–∏—Ö –ª–µ–Ω—Ç)
                        logger.debug(
                            f"–ù–µ—Ç –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è {entry.get('title', 'Unknown')}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Å–≤–µ–∂—É—é"
                        )
                        published_dt = datetime.now()
                    else:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ–±—ä–µ–∫—Ç datetime
                        published_dt = datetime(*published[:6])

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–∞—Ç—å—è —Å–≤–µ–∂–∞—è –∏ –æ—Ç –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    if published_dt > cutoff and self.is_valid_article(entry.link):
                        news.append({"title": entry.title, "link": entry.link})
                        logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {entry.title[:50]}...")
                    elif published_dt <= cutoff:
                        logger.debug(f"–°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è: {entry.title[:50]}...")

                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ RSS: {e}")
                    continue

            logger.debug(f"RSS {feed_url}: –Ω–∞–π–¥–µ–Ω–æ {len(news)} —Å–≤–µ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π")
            return news

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS {feed_url}: {e}")
            # –ü–æ–ø—ã—Ç–∫–∞ —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            try:
                logger.debug(
                    f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏: {feed_url}"
                )

                # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ SSL –ø—Ä–æ–≤–µ—Ä–æ–∫
                feed = feedparser.parse(feed_url)
                if feed.entries:
                    logger.debug(
                        f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏"
                    )
                    # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ –≤ —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º
                    valid_entries = []
                    for entry in feed.entries[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5
                        if hasattr(entry, "title") and hasattr(entry, "link"):
                            valid_entries.append(
                                {"title": entry.title, "link": entry.link}
                            )
                    return valid_entries
            except Exception as e2:
                logger.error(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Ç–∞–∫–∂–µ –Ω–µ—É–¥–∞—á–Ω–∞ –¥–ª—è {feed_url}: {e2}")

            return []

    def extract_summary(self, url: str) -> Optional[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –µ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É newspaper3k –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, –ø–∞—Ä—Å–∏–Ω–≥–∞
        –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∞–º–º–∞—Ä–∏ —Å—Ç–∞—Ç—å–∏.

        Args:
            url: URL —Å—Ç–∞—Ç—å–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Returns:
            Dict –∏–ª–∏ None: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'title', 'summary', 'url' –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Å—Ç–∞—Ç—å–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —è–∑—ã–∫–∞
            article = Article(url, language="ru")

            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            article.config.browser_user_agent = (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            article.config.request_timeout = 10

            # –°–∫–∞—á–∏–≤–∞–µ–º HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            article.download()

            # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é (–∏–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
            article.parse()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ NLP
            if not article.title:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è {url}")
                return None

            # –í—ã–ø–æ–ª–Ω—è–µ–º NLP-–æ–±—Ä–∞–±–æ—Ç–∫—É (—Å–æ–∑–¥–∞–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
            try:
                article.nlp()
            except Exception as nlp_error:
                logger.debug(
                    f"NLP –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {nlp_error}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"
                )
                # –ï—Å–ª–∏ NLP –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                if article.text:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ —Å–∞–º–º–∞—Ä–∏
                    sentences = article.text.split(".")[:3]
                    article.summary = ". ".join(sentences).strip() + "."

            # –ï—Å–ª–∏ —Å–∞–º–º–∞—Ä–∏ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
            if not article.summary and article.text:
                article.summary = article.text[:300] + "..."

            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Å–∞–º–º–∞—Ä–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ
            if not article.summary:
                article.summary = f"–°—Ç–∞—Ç—å—è –Ω–∞ —Ç–µ–º—É: {article.title}"

            result = {
                "title": article.title.strip(),
                "summary": article.summary.strip(),
                "url": url,
                "source": "extracted",  # –ë—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ collect_insights
            }

            logger.debug(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å—Ç–∞—Ç—å—è: {article.title[:50]}...")
            return result

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
            return None

    def collect_insights(
        self,
        keywords: List[str],
        google_api: Optional[str] = None,
        google_cse: Optional[str] = None,
        rss_feeds: Optional[List[str]] = None,
        rss_hours: int = 72,
        max_per_source: int = 5,
    ) -> List[Dict[str, str]]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ Google Custom Search API –∏ RSS-–ª–µ–Ω—Ç—ã,
        –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç–µ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

        Args:
            keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            google_api: –ö–ª—é—á Google API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            google_cse: ID Google Custom Search Engine (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            rss_feeds: –°–ø–∏—Å–æ–∫ URL RSS-–ª–µ–Ω—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            rss_hours: –ü–µ—Ä–∏–æ–¥ –≤ —á–∞—Å–∞—Ö –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ RSS (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 72 —á–∞—Å–∞)
            max_per_source: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π —Å –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, —Å–∞–º–º–∞—Ä–∏ –∏ URL
        """
        results = []

        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API
        if google_api and google_cse:
            logger.info(
                f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google CSE –¥–ª—è {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤..."
            )
            google_results_start = len(results)

            for kw in keywords:
                logger.debug(f"–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{kw}'")

                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
                items = self.search_google_news(
                    kw, google_api, google_cse, max_per_source
                )

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
                for item in items:
                    summary = self.extract_summary(item["link"])
                    if summary:
                        summary["source"] = f"Google Search: {kw}"
                        results.append(summary)

            google_results_count = len(results) - google_results_start
            logger.info(f"‚úÖ Google Search: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {google_results_count} —Å—Ç–∞—Ç–µ–π")
        else:
            logger.info("‚è≠Ô∏è  Google –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        # –°–±–æ—Ä –∏–∑ RSS-–ª–µ–Ω—Ç
        if rss_feeds:
            logger.info(f"üì° –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(rss_feeds)} RSS-–ª–µ–Ω—Ç...")
            rss_results_start = len(results)

            for feed_url in rss_feeds:
                logger.debug(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º RSS: {feed_url}")

                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ RSS —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø–µ—Ä–∏–æ–¥–æ–º
                items = self.fetch_rss_headlines(feed_url, rss_hours)

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π —Å –∫–∞–∂–¥–æ–π –ª–µ–Ω—Ç—ã
                processed_count = 0
                for item in items:
                    if processed_count >= max_per_source:
                        break

                    summary = self.extract_summary(item["link"])
                    if summary:
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± RSS –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                        from urllib.parse import urlparse

                        domain = urlparse(feed_url).netloc
                        summary["source"] = f"RSS: {domain}"
                        results.append(summary)
                        processed_count += 1

            rss_results_count = len(results) - rss_results_start
            logger.info(
                f"‚úÖ RSS: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {rss_results_count} —Å—Ç–∞—Ç–µ–π –∏–∑ {len(rss_feeds)} –ª–µ–Ω—Ç"
            )
        else:
            logger.info("‚è≠Ô∏è  RSS –ª–µ–Ω—Ç—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã")

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_results = []
        seen_urls = set()

        for result in results:
            if result["url"] not in seen_urls:
                unique_results.append(result)
                seen_urls.add(result["url"])

        logger.info(
            f"–°–æ–±—Ä–∞–Ω–æ {len(unique_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (–±—ã–ª–æ {len(results)} –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)"
        )

        return unique_results
